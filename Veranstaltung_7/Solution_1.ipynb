{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9c02df-4507-4656-9acb-f65043deebb4",
   "metadata": {},
   "source": [
    "# Aufgabe 1: Reinforcement Learning mit Policy Network\n",
    "\n",
    "![CartPole](https://www.gymlibrary.dev/_images/cart_pole.gif)\n",
    "\n",
    "[CartPole](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) ist ein Kontrollproblem, bei dem es darum geht, ein auf einem Wagen montiertes Pendel zu balancieren.\n",
    "\n",
    "Es eignet sich als einfaches Einstiegsbeispiel für Reinforcement Learning, da der Zustandsraum mit vier Dimensionen \n",
    "\n",
    "- Position des Wagens,\n",
    "- Geschwindigkeit des Wagens,\n",
    "- Winkel des Pendels, und\n",
    "- Winkelgeschwindigkeit des Pendels\n",
    "\n",
    "sehr klein und die Anzahl der Aktionen\n",
    "\n",
    "- Wagen nach links schieben oder\n",
    "- Wagen nach rechts schieben\n",
    "\n",
    "überschaubar ist.\n",
    "\n",
    "In der ersten Aufgabe geht es darum, das Pendel mit einem einfachen *Policy Network* auszubalancieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2e8ecf4-4cee-454d-9671-9f10c8dfabbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Using cached gym-0.26.2-py3-none-any.whl\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym) (1.22.4)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Using cached gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Installing collected packages: gym-notices, gym\n",
      "Successfully installed gym-0.26.2 gym-notices-0.0.8\n",
      "Collecting pygame\n",
      "  Using cached pygame-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.9 MB)\n",
      "Installing collected packages: pygame\n",
      "Successfully installed pygame-2.1.2\n",
      "Collecting moviepy\n",
      "  Using cached moviepy-1.0.3-py3-none-any.whl\n",
      "Collecting imageio-ffmpeg>=0.2.0\n",
      "  Using cached imageio_ffmpeg-0.4.7-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /opt/conda/lib/python3.10/site-packages (from moviepy) (2.21.3)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from moviepy) (2.28.1)\n",
      "Collecting decorator<5.0,>=4.0.2\n",
      "  Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Collecting proglog<=1.0.0\n",
      "  Using cached proglog-0.1.10-py3-none-any.whl (6.1 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from moviepy) (1.22.4)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /opt/conda/lib/python3.10/site-packages (from moviepy) (4.64.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio<3.0,>=2.5->moviepy) (9.2.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2022.6.15.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.3)\n",
      "Installing collected packages: proglog, imageio-ffmpeg, decorator, moviepy\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.1.1\n",
      "    Uninstalling decorator-5.1.1:\n",
      "      Successfully uninstalled decorator-5.1.1\n",
      "Successfully installed decorator-4.4.2 imageio-ffmpeg-0.4.7 moviepy-1.0.3 proglog-0.1.10\n",
      "Collecting pysdl2\n",
      "  Using cached PySDL2-0.9.14-py3-none-any.whl\n",
      "Installing collected packages: pysdl2\n",
      "Successfully installed pysdl2-0.9.14\n",
      "Collecting pyvirtualdisplay\n",
      "  Using cached PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: pyvirtualdisplay\n",
      "Successfully installed pyvirtualdisplay-3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install pygame\n",
    "!pip install moviepy\n",
    "!pip install pysdl2\n",
    "!pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a5271-2699-4cac-8659-1b05d9eef977",
   "metadata": {},
   "source": [
    "Zunächst erzeugen wir ein *Environment*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd42433-74be-4913-8fe5-13f8f58f2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random \n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from gym import wrappers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3891a0-f9f4-453a-8f3c-c924b0e5c264",
   "metadata": {},
   "source": [
    "## Rendering im Jupyter Notebook\n",
    "\n",
    "Normalerweise benötigt man für das Rendern eine \"richtige\" Anwendung. Wir behelfen uns hier mit `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55f6f9f5-9cb4-4fd1-b50e-3b610bb8c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(env, img):\n",
    "    img.set_data(env.render())\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b32bb37-6b05-4814-a415-717f428d7fdd",
   "metadata": {},
   "source": [
    "## Baseline: `RandomPolicy`\n",
    "\n",
    "Die folgende Policy macht einfach zufällige Aktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a857b711-b64f-4c46-bde6-cf1a85883316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy:\n",
    "    \n",
    "    def __call__(self, observation):\n",
    "        return random.choice([0, 1])\n",
    "    \n",
    "    def update(self, *args):\n",
    "        # Do nothing\n",
    "        pass\n",
    "    \n",
    "    def init_game(self, observation):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb3d47b6-8d9f-4de3-bb1a-ab6186e48c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(policy, episodes=2000, do_render = False, seed=100):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if do_render:\n",
    "        env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    else:\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "    observation, info = env.reset(seed=seed)\n",
    "    policy.init_game(observation)\n",
    "\n",
    "    if do_render:\n",
    "        plt.ion()\n",
    "        plt.axis('off')\n",
    "        img = plt.imshow(env.render())\n",
    "   \n",
    "    status = {}\n",
    "    episode = 0\n",
    "    status['steps'] = 0\n",
    "    status['episode_reward'] = 0\n",
    "    status['average_reward'] = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "\n",
    "    with tqdm(total=episodes) as pbar:\n",
    "        pbar.set_postfix(status)\n",
    "        while True:\n",
    "            try:\n",
    "                action = policy(observation)\n",
    "                observation, reward, terminated, truncated, info = env.step(action)\n",
    "                status['steps'] += 1\n",
    "                status['episode_reward'] += reward\n",
    "                if do_render:\n",
    "                    render(env, img)\n",
    "                policy.update(observation, reward, terminated, truncated, info, pbar)\n",
    "\n",
    "                if terminated or status['steps'] > 1000:\n",
    "                    episode += 1\n",
    "                    if episode > pbar.total:\n",
    "                        break\n",
    "                    total_reward += status['episode_reward']\n",
    "                    status['average_reward'] = 0.05 * status['episode_reward'] + (1 - 0.05) * status['average_reward']\n",
    "                    if status['average_reward'] > env.spec.reward_threshold:\n",
    "                        print(f\"Solved! Running reward is now {status['average_reward']} and \"\n",
    "                              f\"the last episode runs to {status['steps']} time steps!\")\n",
    "                        break\n",
    "\n",
    "                    pbar.set_postfix(status, refresh=episode % 10 == 0)\n",
    "                    pbar.update()\n",
    "                    status['steps'] = 0\n",
    "                    \n",
    "                    status['episode_reward'] = 0\n",
    "                    observation, info = env.reset()\n",
    "                    policy.init_game(observation)\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b00b3cd-23d3-49ff-b0ff-460a24abba7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy = RandomPolicy()\n",
    "play_game(policy, episodes=10, do_render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a25a7c1-da09-445e-8366-2a682e8c68d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 2),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.policy(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75a55c17-7491-473d-957b-86ba43f44da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob'])\n",
    "    \n",
    "class SimplePolicy:\n",
    "    \n",
    "    def __init__(self, gamma=0.99, lr=5e-3):\n",
    "        # Two possible actions 0, 1\n",
    "        self.ACTIONS = [0, 1]       \n",
    "        self.net = PolicyNetwork()\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=lr)\n",
    "        self.mean_reward = None\n",
    "        self.games = 0\n",
    "        self.gamma = gamma\n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    "        \n",
    "    def __call__(self, observation):\n",
    " \n",
    "        probs = self.net(torch.tensor(observation))\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        \n",
    "        self.memory.append(SavedAction(m.log_prob(action)))\n",
    "        \n",
    "        return self.ACTIONS[action.item()]\n",
    "        \n",
    "    def init_game(self, observation):\n",
    "        self.memory = []\n",
    "        self.rewards = []\n",
    "        self.total_reward = 0\n",
    "        \n",
    "        \n",
    "    def discount_rewards(self, r):\n",
    "        discounted = torch.zeros(len(r))\n",
    "        summe = 0\n",
    "        for t in reversed(range(0, len(r))):\n",
    "            summe = summe * self.gamma + r[t]\n",
    "            discounted[t] = summe\n",
    "        return discounted\n",
    "        \n",
    "    def update(self, observation, reward, terminated, truncated, info, status):\n",
    "        self.total_reward += reward\n",
    "        self.rewards.append(reward)\n",
    "        if terminated:\n",
    "            self.games += 1\n",
    "            if self.mean_reward is None:\n",
    "                self.mean_reward = self.total_reward\n",
    "            else:\n",
    "                self.mean_reward = self.mean_reward * 0.95 + self.total_reward * (1.0 - 0.95)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "                \n",
    "            # calculate discounted reward and make it normal distributed\n",
    "            discounted = []\n",
    "            R = 0\n",
    "            for r in self.rewards[::-1]:\n",
    "                R = r + self.gamma * R\n",
    "                discounted.insert(0, R)\n",
    "            discounted = torch.tensor(discounted)\n",
    "            discounted = (discounted - discounted.mean()) / (discounted.std() + self.eps)\n",
    "            \n",
    "            policy_losses = []\n",
    "            for mem, discounted_reward in zip(self.memory, discounted):\n",
    "                policy_losses.append(-(mem.log_prob * discounted_reward))\n",
    "                \n",
    "            loss = torch.stack(policy_losses).sum()\n",
    "            loss.backward()    \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if self.games % 1000 == 0:\n",
    "                self.save(f\"model_{self.games}.pt\")\n",
    "    \n",
    "    \n",
    "    def load(self, PATH):\n",
    "        checkpoint = torch.load(PATH)\n",
    "        self.net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.games = checkpoint['games']\n",
    "        self.mean_reward = checkpoint['mean_reward']\n",
    "        \n",
    "    def save(self, PATH):\n",
    "        torch.save({\n",
    "                    'games': self.games,\n",
    "                    'model_state_dict': self.net.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'mean_reward': self.mean_reward}, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cb88408-d74a-4c1f-a8a0-00ad3f065052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e6d4a565584562b6c86ccec6c51b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved! Running reward is now 501.065650159469 and the last episode runs to 1001 time steps!\n"
     ]
    }
   ],
   "source": [
    "policy = SimplePolicy()\n",
    "play_game(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470589e7-06b7-4228-8416-c8da712e14f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e08835-f835-4e51-b342-673eead6e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size=32):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 2),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        p = self.policy(x)\n",
    "        v = self.critic(x)\n",
    "        return p, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19dcfe6d-042e-4474-9b6a-80eb29e1c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "    \n",
    "class ACPolicy:\n",
    "    \n",
    "    def __init__(self, gamma=0.99, lr=5e-3):\n",
    "        # Two possible actions 0, 1\n",
    "        self.ACTIONS = [0, 1]       \n",
    "        self.net = ActorCriticNetwork()\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=lr)\n",
    "        self.mean_reward = None\n",
    "        self.games = 0\n",
    "        self.gamma = gamma\n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "        \n",
    "    def __call__(self, observation):\n",
    " \n",
    "        probs, value = self.net(torch.tensor(observation))\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        \n",
    "        self.memory.append(SavedAction(m.log_prob(action), value))\n",
    "        self.last_observation = observation\n",
    "        \n",
    "        return self.ACTIONS[action.item()]\n",
    "        \n",
    "    def init_game(self, observation):\n",
    "        self.memory = []\n",
    "        self.rewards = []\n",
    "        self.total_reward = 0\n",
    "        \n",
    "        \n",
    "    def update(self, observation, reward, terminated, truncated, info, status):\n",
    "        self.total_reward += reward\n",
    "        self.rewards.append(reward)\n",
    "        if terminated:\n",
    "            self.games += 1\n",
    "            if self.mean_reward is None:\n",
    "                self.mean_reward = self.total_reward\n",
    "            else:\n",
    "                self.mean_reward = self.mean_reward * 0.95 + self.total_reward * (1.0 - 0.95)\n",
    "                \n",
    "            # calculate discounted reward and make it normal distributed\n",
    "            discounted = []\n",
    "            R = 0\n",
    "            for r in self.rewards[::-1]:\n",
    "                R = r + self.gamma * R\n",
    "                discounted.insert(0, R)\n",
    "            discounted = torch.tensor(discounted)\n",
    "            #discounted = (discounted - discounted.mean()) / (discounted.std() + self.eps)\n",
    "            \n",
    "            policy_losses = []\n",
    "            value_losses = []\n",
    "            for mem, discounted_reward in zip(self.memory, discounted):\n",
    "                advantage = discounted_reward - mem.value.item() \n",
    "                policy_losses.append(-(mem.log_prob * advantage))\n",
    "                \n",
    "                value_losses.append(F.smooth_l1_loss(mem.value, discounted_reward.unsqueeze(0)))\n",
    "               \n",
    "            self.optimizer.zero_grad()\n",
    "            loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "            loss.backward()    \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if self.games % 1000 == 0:\n",
    "                self.save(f\"model_{self.games}.pt\")\n",
    "    \n",
    "    \n",
    "    def load(self, PATH):\n",
    "        checkpoint = torch.load(PATH)\n",
    "        self.net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.games = checkpoint['games']\n",
    "        self.mean_reward = checkpoint['mean_reward']\n",
    "        \n",
    "    def save(self, PATH):\n",
    "        torch.save({\n",
    "                    'games': self.games,\n",
    "                    'model_state_dict': self.net.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'mean_reward': self.mean_reward}, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5adc6c96-3282-4238-868a-f420c4aed689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0b3fc967d94fd580b6090611753d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved! Running reward is now 477.5831251868573 and the last episode runs to 608 time steps!\n"
     ]
    }
   ],
   "source": [
    "policy = ACPolicy()\n",
    "play_game(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6ff7d-c9ef-48b1-bd6f-79f2a7a5a53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "play_game(policy, do_render = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2036415e-bd36-49c5-bf4d-38b1c8c748ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
