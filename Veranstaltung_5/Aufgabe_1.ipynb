{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Style Transfer\n",
    "\n",
    "In dieser Aufgabe geht es darum, *Neural Style Transfer* einmal selbst auszuprobieren und zu testen, was passiert, wenn man\n",
    "\n",
    "- die Parameter des Style-Losses (ausgewählte Layer und Gewichtungen),\n",
    "- die Parameter des Content-Losses \n",
    "\n",
    "anpasst.\n",
    "\n",
    "### Zutaten\n",
    "\n",
    "Als Ausgangspunkt verwenden wir\n",
    "\n",
    "- ein vortrainiertes VGG19,\n",
    "- ein Bild der Fachhochschule Südwestfalen, und\n",
    "- das Gemälde *De Sterrennacht* (Sternennacht) von Vincent van Gogh.\n",
    "\n",
    "Auch hier können Sie mit eigenen Bildern oder anderen vortrainierten Modellen experimentieren.\n",
    "\n",
    "Der folgende Code entspricht weitgehend dem Beispiel zum Buch, enthält aber ein paar Aktualisierungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "from functools import reduce\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 512\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "preprocessing = transforms.Compose([transforms.Resize(image_size),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), \n",
    "                           transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], \n",
    "                                                std=[1,1,1]),\n",
    "                           transforms.Lambda(lambda x: x.mul_(255)),\n",
    "                          ])\n",
    "processing = transforms.Compose([transforms.Lambda(lambda x: x.mul_(1./255)),\n",
    "                           transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961], \n",
    "                                                std=[1,1,1]),\n",
    "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), \n",
    "                           ])\n",
    "postprocess = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "def postprocess_b(tensor): \n",
    "    t = processing(tensor)\n",
    "    t[t>1] = 1    \n",
    "    t[t<0] = 0\n",
    "    img = postprocess(t)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IPImage\n",
    "IPImage('content/dab21-fh-swf-iserlohn.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = Variable(preprocessing(image))\n",
    "    # fake batch dimension required to fit network's input dimensions\n",
    "    image = image.unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auswertung der Layer\n",
    "\n",
    "Während des Optimierungsvorgangs möchten wir auf die Ausgaben einzelner Layer zugreifen. Dazu wird an den entsprechenden Layern mit der Methode `register_forward_hook()` eine Funktion registriert, die Ausgabewerte \"mitschneidet\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerActivations():\n",
    "    features=[]\n",
    "    \n",
    "    def __init__(self, model, layer_nums):\n",
    "        \n",
    "        self.hooks = []\n",
    "        for layer_num in layer_nums:\n",
    "            self.hooks.append(model[layer_num].register_forward_hook(self.hook_fn))\n",
    "    \n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.features.append(output)\n",
    "\n",
    "    \n",
    "    def remove(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_layers(layers, img, model=None):\n",
    "    la = LayerActivations(model,layers)\n",
    "    #Clearing the cache \n",
    "    la.features = []\n",
    "    out = model(img)\n",
    "    la.remove()\n",
    "    return la.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GramMatrix(nn.Module):\n",
    "    \n",
    "    def forward(self,input):\n",
    "        b,c,h,w = input.size()\n",
    "        features = input.view(b,c,h*w)\n",
    "        gram_matrix =  torch.bmm(features,features.transpose(1,2))\n",
    "        gram_matrix.div_(h*w)\n",
    "        return gram_matrix\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleLoss(nn.Module):\n",
    "    \n",
    "    def forward(self,inputs,targets):\n",
    "        out = nn.MSELoss()(GramMatrix()(inputs),targets)\n",
    "        return (out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laden eines Modells\n",
    "\n",
    "Wir laden hier für die Featureextraktion ein vortrainiertes VGG19. Dieses Modell soll nicht trainiert werden, daher wird die Gradientenberechnung deaktiviert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = vgg19(weights=VGG19_Weights.DEFAULT).features\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style-Transfer durch Optimierung des Bildes\n",
    "\n",
    "In der folgenden Funktion finden Sie einige Parameter, etwa die ausgewählten Layer und Gewichte für die beiden Verlustfunktionen.\n",
    "\n",
    "Eine Besonderheit ist der verwendete Optimizer `optim.LBFGS`.  Dieser berechnet eine Näherung der Hesse-Matrix der Verlustfunktion (so etwas wie die zweite Ableitung einer Funktion mehrerer Variablen) und muss die Funktion dazu wiederholt auswerten. Daher erwartet die Funktion `optimizer.step(closure)` als Parameter eine Funktion (\"Closure\"), die die Verlustfunktion berechnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def style_transfer(style_image, content_image, maximum_iterations=1000, model=model):\n",
    "\n",
    "    if is_cuda:\n",
    "        style_image = style_image.cuda()\n",
    "        content_image = content_image.cuda()\n",
    "        model = model.cuda()\n",
    "    \n",
    "    ### Parameters to experment with \n",
    "    style_layers = [1,6,11,20,25]\n",
    "    content_layers = [-1, -5, -15]\n",
    "    \n",
    "    style_weights = [1e3/n**2 for n in [64, 128, 256, 512, 512]]\n",
    "    content_weights = [1e0, 1e0, 1e0]\n",
    "    ####\n",
    "    \n",
    "    loss_layers = style_layers + content_layers\n",
    "    weights = style_weights + content_weights\n",
    "    \n",
    "    content_targets = extract_layers(content_layers, content_image, model=model)\n",
    "    content_targets = [t.detach() for t in content_targets]\n",
    "    \n",
    "    style_targets = extract_layers(style_layers, style_image, model=model)\n",
    "    style_targets = [GramMatrix()(t).detach() for t in style_targets]\n",
    "\n",
    "    # this is a fixed target with content from the content image and style from the style image\n",
    "    targets = style_targets + content_targets\n",
    "    \n",
    "    loss_fns = [StyleLoss()] * len(style_layers) + [nn.MSELoss()] * len(content_layers)\n",
    "    if is_cuda:\n",
    "        loss_fns = [fn.cuda() for fn in loss_fns]\n",
    "\n",
    "    output_image = Variable(content_image.data.clone(),requires_grad=True)\n",
    "\n",
    "    # run style transfer\n",
    "    optimizer = optim.LBFGS([output_image]);\n",
    "    \n",
    "    with tqdm(total=maximum_iterations) as pbar:\n",
    "        while pbar.n < pbar.total:\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                out = extract_layers(loss_layers, output_image, model=model)\n",
    "                layer_losses = [weights[a] * loss_fns[a](A, targets[a]) for a, A in enumerate(out)]\n",
    "                loss = reduce(torch.add, layer_losses)\n",
    "                loss.backward()\n",
    "                pbar.update()\n",
    "                pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "                return loss\n",
    "\n",
    "            optimizer.step(closure)\n",
    "\n",
    "    out_img_hr = postprocess_b(output_image.data[0].cpu().squeeze())\n",
    "\n",
    "    plt.imshow(out_img_hr)\n",
    "    ax = plt.gca()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    plt.gcf().set_size_inches(10, 10)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image = loader(\"style/starry_night.jpg\")\n",
    "content_image = loader(\"content/dab21-fh-swf-iserlohn.jpg\")\n",
    "\n",
    "style_transfer(style_image, content_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weitere Experimente\n",
    "\n",
    "Probieren Sie den Neural Style Transfer mit anderen Bildern und anderen Parametern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
