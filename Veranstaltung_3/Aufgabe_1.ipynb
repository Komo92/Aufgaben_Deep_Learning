{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textklassifikation mit RNN: GermEval 2018\n",
    "\n",
    "*GermEval* – für German Evaluation – ist ein jährlicher Wettbewerb im Bereich Natural Language Processing für deutschsprachige Texte (s. [https://germeval.github.io/](https://germeval.github.io/)).\n",
    "\n",
    "Im Jahr 2018 ging es um die Erkennung von Beleidigungen in deutschsprachigen Tweets.\n",
    "\n",
    "In dieser Aufgabe wollen wir Rekurrente Neuronale Netze (RNN) zur Klassifikation nutzen. Zunächst einmal starten wir mit Vorarbeiten.\n",
    "\n",
    "## Format der Daten\n",
    "\n",
    "Die Trainings- und Testdaten liegen als mit Tabulatoren separierte Textdateien (Tab Separated Values – TSV) vor. Uns interessieren die erste Spalte (der Tweet) und die zweite Spalte (`OFFENSE` für Beleidigung bzw. `OTHER` für keine Beleidigung)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -10 texts/germeval2018.training.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesen der Daten\n",
    "\n",
    "Für das Einlesen der Datensätze verwenden wir die Klasse `NamedTuple`, mit der sich die Daten einfach speichern lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Record(text='@corinnamilborn Liebe Corinna, wir würden dich gerne als Moderatorin für uns gewinnen! Wärst du begeisterbar?', primary_label='OTHER', secondary_label='OTHER'),\n",
       " Record(text='@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverständlich. Dass das BVerfG Sachleistungen nicht ausschließt, kritisieren wir.', primary_label='OTHER', secondary_label='OTHER'),\n",
       " Record(text='@ahrens_theo fröhlicher gruß aus der schönsten stadt der welt theo ⚓️', primary_label='OTHER', secondary_label='OTHER'),\n",
       " Record(text='@dushanwegner Amis hätten alles und jeden gewählt...nur Hillary wollten sie nicht und eine Fortsetzung von Obama-Politik erst recht nicht..!', primary_label='OTHER', secondary_label='OTHER'),\n",
       " Record(text='@spdde kein verläßlicher Verhandlungspartner. Nachkarteln nach den Sondierzngsgesprächen - schickt diese Stümper #SPD in die Versenkung.', primary_label='OFFENSE', secondary_label='INSULT')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Record = namedtuple('Record', [ 'text', 'primary_label', 'secondary_label' ])\n",
    "\n",
    "with open('texts/germeval2018.training.tsv', 'r') as file:\n",
    "    training_data = [ Record(*line[:-1].split('\\t')) for line in file ]\n",
    "\n",
    "with open('texts/germeval2018.test.tsv', 'r') as file:\n",
    "    test_data = [ Record(*line[:-1].split('\\t')) for line in file ]\n",
    "\n",
    "training_data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Überblick über die Daten\n",
    "\n",
    "Wir schauen uns die Verteilung der Kategorien in den Trainings- und Testdaten an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('OTHER', 'OTHER'): 3321,\n",
       "         ('OFFENSE', 'INSULT'): 595,\n",
       "         ('OFFENSE', 'PROFANITY'): 71,\n",
       "         ('OFFENSE', 'ABUSE'): 1022})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter([ (record.primary_label, record.secondary_label) for record in training_data ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('OTHER', 'OTHER'): 2330,\n",
       "         ('OFFENSE', 'ABUSE'): 773,\n",
       "         ('OFFENSE', 'INSULT'): 381,\n",
       "         ('OFFENSE', 'PROFANITY'): 48})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([ (record.primary_label, record.secondary_label) for record in test_data ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchtext spacy\n",
    "!python -m spacy download de_core_news_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing der Tweets\n",
    "\n",
    "Für die weitere Verarbeitung wollen wir Twitter Handles (`@username`) löschen und das Hashtag-Zeichen entfernen. Damit verhindern wir, dass unser Model später die Namen auswendig lernt, um die Daten zu klassifizieren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' kein verläßlicher Verhandlungspartner. Nachkarteln nach den Sondierzngsgesprächen   schickt diese Stümper SPD in die Versenkung.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_tweet(text):\n",
    "    \"\"\" Preprocess and tokenize a tweet. \"\"\"\n",
    "    \n",
    "    # remove handles, i.e. @username\n",
    "    text = re.sub('\\@\\w+', '', text)\n",
    "    # remove hashtags, quotes, etc.\n",
    "    text = re.sub('[\\#\"\\']+', '', text)\n",
    "    text = text.replace('-', ' ')\n",
    "    return text\n",
    "\n",
    "clean_tweet(training_data[4].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vektorisierung mit vortrainierten Wortvektoren\n",
    "\n",
    "Wir nutzen vortrainierte Wortvektoren aus Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "def vectorize(text):\n",
    "    \"\"\"Vectorize text using the German SpaCy tokenizer\"\"\"\n",
    "    return torch.Tensor(np.array([tok.vector for tok in nlp(clean_tweet(text)) if tok.has_vector ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 300])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize(training_data[0].text).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden der Daten\n",
    "\n",
    "Mithilfe der Funktion `vectorize()` definieren wir die Funktion `collate_batch()`, die einen Batch in zwei Tensoren – für die Label und die Texte – umwandelt.\n",
    "Damit wir das RNN später effizient trainieren können, bringen wir die Text-Tensoren mithilfe der Funktion `pad_sequence()` auf die gleiche Länge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "LABEL = { 'OFFENSE': 1, 'OTHER': 0 }\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    \n",
    "    for record in batch:\n",
    "        label_list.append(LABEL[record.primary_label])\n",
    "        processed_text = vectorize(record.text)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.shape[0])\n",
    "    return torch.tensor(label_list), pad_sequence(text_list, batch_first=True), lengths\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True, num_workers=5, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True, num_workers=5, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64]), torch.Size([64, 49, 300]), 64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, texts, lengths = next(iter(train_dataloader))\n",
    "labels.shape, texts.shape, len(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klassifikation von Text mittels RNNs\n",
    "\n",
    "Texte bestehen aus einer *Folge* von Wörtern. \n",
    "Rekurrente Neuronale Netze (RNNs) eignen sich gut für die Verarbeitung von Folgen.\n",
    "\n",
    "Unser Netz wird dabei aus zwei Schichten bestehen:\n",
    "1. das eigentliche RNN aus *Long-Short-Term-Memoy (LSTM)* Zellen oder *Gated Recurrent Units (GRU)*, die die Wortfolge auf eine Folge von *Zuständen* abbilden,\n",
    "2. einen linearen Layer, der den letzten Zustand auf eine eindimensionale Variable abbildet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufbau des RNNs\n",
    "\n",
    "Nun bauen wir das oben beschriebene Netz aus Embedding Layer, RNN Layer und Linear Layer auf.\n",
    "\n",
    "Die Funktionen `torch.nn.utils.rnn.pack_padded_sequence` und `torch.nn.utils.rnn.pad_packed_sequence` packen bzw. entpacken die Tensoren für eine effiziente Berechnung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class RNN(torch.nn.Module) :\n",
    "    def __init__(self, hidden_dim = 20, embedding_dim = 300, dropout = 0.4) :\n",
    "        super().__init__()\n",
    "  \n",
    "        #self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, dropout=0.1, batch_first=True)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=3, dropout=dropout, batch_first=True, bidirectional=False)\n",
    "        self.linear = nn.Linear(hidden_dim, 2)\n",
    "    \n",
    "        \n",
    "    def forward(self, _x, **kwargs):\n",
    "        (x, input_lengths) = _x\n",
    "        \n",
    "        x = pack_padded_sequence(x, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        #x, (ht, ct) = self.lstm(x)\n",
    "        x, ht = self.gru(x)\n",
    "        x, output_lengths = pad_packed_sequence(x, batch_first=True)\n",
    "        #print(ht[-1].shape)\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training und Validierung\n",
    "\n",
    "Der Einfachheit halber verwenden wir für Training und Validierung weiterhin Scikit Learn und binden das RNN als `skorch.NeuralNetBinaryClassifier` ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(hidden_dim=64, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120322"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer=torch.optim.AdamW(model.parameters(), lr=0.0005)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a885d8fd5a34ab29a5f1f6d6f850ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fdd13ddedd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "Exception ignored in:     <function _MultiProcessingDataLoaderIter.__del__ at 0x7fdd13ddedd0>if w.is_alive():\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError    : self._shutdown_workers()can only test a child process\n",
      "\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child processException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fdd13ddedd0>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fdd13ddedd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fdd13ddedd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fdd13ddedd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fdd13ddedd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fdd13ddedd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "Exception ignored in:   File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7fdd13ddedd0>    \n",
      "if w.is_alive():Traceback (most recent call last):\n",
      "\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "        self._shutdown_workers()assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "\n",
      "AssertionError    : if w.is_alive():can only test a child process\n",
      "\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fdd13ddedd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fdd13ddedd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fdd13ddedd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fdd13ddedd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fdd13ddedd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'Exception ignored in: \n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7fdd13ddedd0>AssertionError\n",
      ": Traceback (most recent call last):\n",
      "can only test a child process  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fdd13ddedd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fdd13ddedd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fdd13ddedd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "Exception ignored in:     assert self._parent_pid == os.getpid(), 'can only test a child process'<function _MultiProcessingDataLoaderIter.__del__ at 0x7fdd13ddedd0>\n",
      "\n",
      "AssertionErrorTraceback (most recent call last):\n",
      ":   File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "can only test a child process\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fdd13ddedd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "epochs = 10 \n",
    "training_loss = []\n",
    "testing_loss = []\n",
    "training_acc = []\n",
    "testing_acc = []\n",
    "\n",
    "\n",
    "with tqdm(range(epochs)) as iterator:\n",
    "    for epoch in iterator:\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0     \n",
    "        for idx, (target, text, length) in enumerate(train_dataloader):\n",
    "\n",
    "            target, text = target.to(device), text.to(device)\n",
    "\n",
    "            optimizer.zero_grad() \n",
    "            output = model((text, length))\n",
    "\n",
    "            loss = loss_fn(output, target) \n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss_fn(output, target).item()\n",
    "            predictions = output.data.max(1)[1]\n",
    "            train_acc += (predictions == target).sum().item()\n",
    " \n",
    "        training_acc.append(train_acc/len(train_dataloader.dataset))\n",
    "        training_loss.append(train_loss/len(train_dataloader.dataset))\n",
    "            \n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for target, text, length in test_dataloader:\n",
    "                target, text = target.to(device), text.to(device)\n",
    "                output = model((text, length))\n",
    "                loss = loss_fn(output, target)\n",
    "                prediction = torch.argmax(output, 1)\n",
    "                test_acc += (prediction == target).sum().item()\n",
    "                test_loss += loss.item()        \n",
    "                \n",
    "            testing_acc.append(test_acc/len(test_dataloader.dataset))\n",
    "            testing_loss.append(test_loss/len(test_dataloader.dataset))\n",
    "            \n",
    "        loss = running_loss/count\n",
    "        accuracy = 100. * running_correct/count \n",
    "        iterator.set_postfix_str(f\"train_acc: {train_acc/len(train_dataloader.dataset):.2f} test_acc: {test_acc/len(test_dataloader.dataset):.2f} train_loss: {train_loss/len(train_dataloader.dataset):.2e} test_loss: {test_loss/len(test_dataloader.dataset):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
